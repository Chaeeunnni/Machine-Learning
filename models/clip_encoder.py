from PIL import Image
import torch
import clip
import os
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


class CLIPEncoder:
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.model, self.preprocess = clip.load("ViT-B/32", device=device)
        self.max_length = 77  # CLIPì˜ ìµœëŒ€ í† í° ê¸¸ì´

    def encode_image(self, image_path):
        image = self.preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)
        with torch.no_grad():
            image_features = self.model.encode_image(image)
        return image_features.cpu().numpy()[0]

    def truncate_korean_text(self, text, max_tokens=70):
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ CLIP í† í° ê¸¸ì´ì— ë§ê²Œ ìë¥´ê¸°"""
        # ê¸°ë³¸ ì •ë¦¬
        text = text.strip()

        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not text:
            return "ê¸°ë³¸ í…ìŠ¤íŠ¸"

        # ëŒ€í™” í˜•ì‹ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if 'A:' in text and 'B:' in text:
            # ë§ˆì§€ë§‰ ë°œí™”ë§Œ ì‚¬ìš© (ê°€ì¥ ìµœê·¼ ê°ì • ìƒíƒœ)
            parts = re.split(r'[AB]:', text)
            if len(parts) > 1:
                text = parts[-1].strip()

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        sentences = re.split(r'[.!?]', text)
        truncated = ""

        for sentence in sentences:
            if not sentence.strip():
                continue

            test_text = truncated + sentence.strip() + "."
            # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ëŠ” ë³´í†µ 1.5-2ìë‹¹ 1í† í°)
            estimated_tokens = len(test_text) // 1.5

            if estimated_tokens <= max_tokens:
                truncated = test_text
            else:
                break

        # ë¹ˆ ê²°ê³¼ ë°©ì§€
        if not truncated.strip():
            # ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ë³´ì¥ (ê¸€ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°)
            char_limit = int(max_tokens * 1.5)  # í† í°ë‹¹ 1.5ì ì¶”ì •
            if len(text) > char_limit:
                truncated = text[:char_limit] + "..."
            else:
                truncated = text

        return truncated.strip() if truncated.strip() else "ê¸°ë³¸ í…ìŠ¤íŠ¸"

    def get_text_embedding(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not text or not isinstance(text, str):
                print(f"ì˜ëª»ëœ í…ìŠ¤íŠ¸ ì…ë ¥: {text}")
                text = "ê¸°ë³¸ í…ìŠ¤íŠ¸"

            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ê¸¸ì´ ì œí•œ
            processed_text = self.truncate_korean_text(text)

            # CLIP í† í°í™” ì‹œë„ (truncate=True ì˜µì…˜ ì‚¬ìš©)
            text_tokens = clip.tokenize([processed_text], truncate=True).to(self.device)

            with torch.no_grad():
                text_features = self.model.encode_text(text_tokens)
                # ì •ê·œí™”
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            result = text_features.cpu().numpy()[0]
            if not isinstance(result, np.ndarray):
                result = np.array(result)

            if result.ndim == 0:
                result = result.reshape(1)

            return result.astype(np.float32)

        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {text}")
            print(f"ì˜¤ë¥˜: {e}")

            # í´ë°± 1: ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„
            try:
                fallback_text = text[:20] if len(text) > 20 else text
                if not fallback_text.strip():
                    fallback_text = "ê¸°ë³¸"

                text_tokens = clip.tokenize([fallback_text], truncate=True).to(self.device)

                with torch.no_grad():
                    text_features = self.model.encode_text(text_tokens)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)

                print(f"í´ë°± ì„±ê³µ: {fallback_text}")
                return text_features.cpu().numpy()[0]

            except Exception as e2:
                print(f"í´ë°±ë„ ì‹¤íŒ¨: {e2}")

                # í´ë°± 2: ì˜ì–´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                try:
                    default_text = "default text"
                    text_tokens = clip.tokenize([default_text], truncate=True).to(self.device)

                    with torch.no_grad():
                        text_features = self.model.encode_text(text_tokens)
                        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

                    print(f"ì˜ì–´ ê¸°ë³¸ í…ìŠ¤íŠ¸ë¡œ í´ë°± ì„±ê³µ")
                    return text_features.cpu().numpy()[0]

                except Exception as e3:
                    print(f"ëª¨ë“  í´ë°± ì‹¤íŒ¨: {e3}")
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ì˜ë²¡í„° ë°˜í™˜ (CLIP ì„ë² ë”© ì°¨ì›ì€ 512)
                    return np.zeros(512, dtype=np.float32)

    def get_dual_similarities(self, text_emb, image_embeddings, emotions=None, situations=None):
        """
        ë“€ì–¼ ìœ ì‚¬ë„ ê³„ì‚° - í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ê°ì •/ìƒí™© ê°€ì¤‘ì¹˜ë¥¼ ê²°í•©

        Args:
            text_emb: í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„°
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´
            emotions: ê°ì§€ëœ ê°ì • ë¦¬ìŠ¤íŠ¸
            situations: ê°ì§€ëœ ìƒí™© ë¦¬ìŠ¤íŠ¸

        Returns:
            combined_similarities: ê²°í•©ëœ ìœ ì‚¬ë„ ì ìˆ˜
            text_similarities: ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            weighted_similarities: ê°€ì¤‘ì¹˜ ì ìš©ëœ ìœ ì‚¬ë„
        """
        try:
            # 1. ì…ë ¥ íƒ€ì… ê²€ì¦
            if isinstance(text_emb, str):
                print(f"[!] get_dual_similarities: í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ë¬¸ìì—´ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ë² ë”© ìƒì„±...")
                text_emb = self.get_text_embedding(text_emb)

            if text_emb is None:
                print(f"[!] get_dual_similarities: í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ Noneì…ë‹ˆë‹¤.")
                return np.zeros(len(image_embeddings)), np.zeros(len(image_embeddings)), np.zeros(len(image_embeddings))

            # numpy ë°°ì—´ í™•ì¸
            if not isinstance(text_emb, np.ndarray):
                text_emb = np.array(text_emb)


            # 1. ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            text_similarities = self._calculate_text_similarity(text_emb, image_embeddings)

            # 2. ê°ì •/ìƒí™© ê°€ì¤‘ì¹˜ ê³„ì‚°
            emotion_weights = self._calculate_emotion_weights(emotions) if emotions else 1.0
            situation_weights = self._calculate_situation_weights(situations) if situations else 1.0

            # 3. ê°€ì¤‘ì¹˜ ì ìš©ëœ ìœ ì‚¬ë„
            weighted_similarities = text_similarities * emotion_weights * situation_weights

            # 4. ê²°í•©ëœ ìµœì¢… ìœ ì‚¬ë„ (í…ìŠ¤íŠ¸ 70% + ê°€ì¤‘ì¹˜ 30%)
            combined_similarities = (0.7 * text_similarities + 0.3 * weighted_similarities)

            return combined_similarities, text_similarities, weighted_similarities

        except Exception as e:
            print(f"[!] ë“€ì–¼ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë§Œ ë°˜í™˜
            text_similarities = self._calculate_text_similarity(text_emb, image_embeddings)
            return text_similarities, text_similarities, text_similarities

    def _calculate_text_similarity(self, text_emb, image_embeddings):
        """ìˆœìˆ˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # 1. ì…ë ¥ íƒ€ì… ê²€ì¦ ë° ë³€í™˜
            if text_emb is None:
                print("[!] í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ Noneì…ë‹ˆë‹¤")
                return np.zeros(len(image_embeddings))

            # ë¬¸ìì—´ì´ë‚˜ ì˜ëª»ëœ íƒ€ì…ì´ ë“¤ì–´ì˜¨ ê²½ìš° ì²˜ë¦¬
            if isinstance(text_emb, str):
                print(f"[!] í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ë¬¸ìì—´ì…ë‹ˆë‹¤: {text_emb[:50]}...")
                print("[!] ë¬¸ìì—´ì„ ë‹¤ì‹œ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤")
                text_emb = self.get_text_embedding(text_emb)
                if text_emb is None:
                    return np.zeros(len(image_embeddings))

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            if not isinstance(text_emb, np.ndarray):
                try:
                    text_emb = np.array(text_emb)
                except:
                    print(f"[!] í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {type(text_emb)}")
                    return np.zeros(len(image_embeddings))

            # 2. ì°¨ì› ê²€ì¦
            if text_emb.size == 0:
                print("[!] ë¹ˆ í…ìŠ¤íŠ¸ ì„ë² ë”©")
                return np.zeros(len(image_embeddings))

            if len(image_embeddings) == 0:
                print("[!] ë¹ˆ ì´ë¯¸ì§€ ì„ë² ë”©")
                return np.array([])

            # 3. ì°¨ì› ì¡°ì •
            if text_emb.ndim == 1:
                text_emb = text_emb.reshape(1, -1)  # (1, 512)
            elif text_emb.ndim > 2:
                print(f"[!] í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {text_emb.shape}")
                text_emb = text_emb.reshape(1, -1)

            # 4. ì´ë¯¸ì§€ ì„ë² ë”©ë„ ê²€ì¦
            if not isinstance(image_embeddings, np.ndarray):
                try:
                    image_embeddings = np.array(image_embeddings)
                except:
                    print(f"[!] ì´ë¯¸ì§€ ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {type(image_embeddings)}")
                    return np.zeros(text_emb.shape[0])

            # 5. ì°¨ì› í˜¸í™˜ì„± ê²€ì‚¬
            if text_emb.shape[1] != image_embeddings.shape[1]:
                print(f"[!] ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: í…ìŠ¤íŠ¸ {text_emb.shape}, ì´ë¯¸ì§€ {image_embeddings.shape}")
                # ì°¨ì› ë§ì¶”ê¸° ì‹œë„
                min_dim = min(text_emb.shape[1], image_embeddings.shape[1])
                text_emb = text_emb[:, :min_dim]
                image_embeddings = image_embeddings[:, :min_dim]

            # 6. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(text_emb, image_embeddings)[0]  # (n_images,)

            # 7. NaN ë° ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
            similarities = np.nan_to_num(similarities, nan=0.0, posinf=1.0, neginf=0.0)

            return similarities

        except Exception as e:
            print(f"[!] í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            print(f"[!] í…ìŠ¤íŠ¸ ì„ë² ë”© íƒ€ì…: {type(text_emb)}")
            if hasattr(text_emb, 'shape'):
                print(f"[!] í…ìŠ¤íŠ¸ ì„ë² ë”© shape: {text_emb.shape}")
            print(f"[!] ì´ë¯¸ì§€ ì„ë² ë”© íƒ€ì…: {type(image_embeddings)}")
            if hasattr(image_embeddings, 'shape'):
                print(f"[!] ì´ë¯¸ì§€ ì„ë² ë”© shape: {image_embeddings.shape}")

            # ì•ˆì „í•œ í´ë°±
            if hasattr(image_embeddings, '__len__'):
                return np.zeros(len(image_embeddings))
            else:
                return np.zeros(1)

    def _calculate_emotion_weights(self, emotions):
        """ê°ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if not emotions:
            return 1.0

        # ê°ì •ë³„ ê°€ì¤‘ì¹˜ ë§¤í•‘ (ë” ì •í™•í•œ ë§¤í•‘)
        emotion_weights = {
            'ê¸°ì˜ë‹¤': 1.2,
            'ìŠ¬í”„ë‹¤': 1.1,
            'í™”ë‚˜ë‹¤': 1.15,
            'ë¬´ì„­ë‹¤': 1.1,
            'ë†€ëë‹¤': 1.05,
            'ê°ì‚¬í•˜ë‹¤': 1.1,
            'ìŠ¤íŠ¸ë ˆìŠ¤': 1.2,
            'ê±±ì •': 1.15,
            'í–‰ë³µ': 1.2,
            'ìš°ìš¸': 1.1,
            'ë¶€ë‹´': 1.15,
            'ì§œì¦': 1.15,
            'ë¶ˆì•ˆ': 1.1
        }

        # ê°ì§€ëœ ê°ì •ë“¤ì˜ í‰ê·  ê°€ì¤‘ì¹˜
        weights = [emotion_weights.get(emotion, 1.0) for emotion in emotions]
        return np.mean(weights) if weights else 1.0

    def _calculate_situation_weights(self, situations):
        """ìƒí™© ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        if not situations:
            return 1.0

        # ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ë§¤í•‘ (ë” ì •í™•í•œ ë§¤í•‘)
        situation_weights = {
            'ì§ì¥': 1.15,
            'ê°€ì¡±': 1.1,
            'ì—°ì• ': 1.1,
            'ì¹œêµ¬': 1.05,
            'í•™êµ': 1.1,
            'ëˆ': 1.2,
            'ê±´ê°•': 1.15,
            'ì·¨ì—…': 1.2,
            'ì¶•í•˜': 1.1,
            'ì¡°ì–¸': 1.05,
            'ê²½ì œ': 1.2,
            'ë…¸í›„': 1.1,
            'ê²°í˜¼': 1.1
        }

        # ê°ì§€ëœ ìƒí™©ë“¤ì˜ í‰ê·  ê°€ì¤‘ì¹˜
        weights = [situation_weights.get(situation, 1.0) for situation in situations]
        return np.mean(weights) if weights else 1.0

    def get_enhanced_text_embedding(self, text, emotions=None, situations=None):
        """
        ê°•í™”ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© - ê°ì •/ìƒí™© ì •ë³´ í¬í•¨

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            emotions: ê°ì§€ëœ ê°ì • ë¦¬ìŠ¤íŠ¸
            situations: ê°ì§€ëœ ìƒí™© ë¦¬ìŠ¤íŠ¸

        Returns:
            enhanced_embedding: ê°•í™”ëœ ì„ë² ë”© ë²¡í„°
        """
        try:
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”©
            base_embedding = self.get_text_embedding(text)

            if not emotions and not situations:
                return base_embedding

            # ê°ì •/ìƒí™© í‚¤ì›Œë“œë¥¼ í…ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ì¬ì„ë² ë”©
            enhanced_text = text

            if emotions:
                emotion_keywords = ' '.join(emotions)
                enhanced_text += f" {emotion_keywords}"

            if situations:
                situation_keywords = ' '.join(situations)
                enhanced_text += f" {situation_keywords}"

            # ê°•í™”ëœ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìƒì„±
            enhanced_embedding = self.get_text_embedding(enhanced_text)

            # ê¸°ë³¸ ì„ë² ë”©ê³¼ ê°•í™”ëœ ì„ë² ë”©ì„ ê²°í•© (7:3 ë¹„ìœ¨)
            combined_embedding = 0.7 * base_embedding + 0.3 * enhanced_embedding

            return combined_embedding

        except Exception as e:
            print(f"[!] ê°•í™”ëœ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return self.get_text_embedding(text)

    def batch_text_similarity(self, text_emb, image_embeddings, batch_size=100):
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)

        Args:
            text_emb: í…ìŠ¤íŠ¸ ì„ë² ë”©
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            similarities: ìœ ì‚¬ë„ ì ìˆ˜ ë°°ì—´
        """
        try:
            n_images = len(image_embeddings)
            similarities = np.zeros(n_images)

            if text_emb.ndim == 1:
                text_emb = text_emb.reshape(1, -1)

            for i in range(0, n_images, batch_size):
                end_idx = min(i + batch_size, n_images)
                batch_embeddings = image_embeddings[i:end_idx]

                batch_similarities = cosine_similarity(text_emb, batch_embeddings)[0]
                similarities[i:end_idx] = batch_similarities

            return np.nan_to_num(similarities, nan=0.0)

        except Exception as e:
            print(f"[!] ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros(len(image_embeddings))

    def debug_similarity_calculation(self, text, text_emb, image_embeddings, top_k=5):
        """
        ìœ ì‚¬ë„ ê³„ì‚° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            text_emb: í…ìŠ¤íŠ¸ ì„ë² ë”©
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ì¶œë ¥
        """
        try:
            similarities = self._calculate_text_similarity(text_emb, image_embeddings)

            print(f"[DEBUG] í…ìŠ¤íŠ¸: {text[:50]}...")
            print(f"[DEBUG] í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›: {text_emb.shape}")
            print(f"[DEBUG] ì´ë¯¸ì§€ ì„ë² ë”© ì°¨ì›: {image_embeddings.shape}")
            print(f"[DEBUG] ìœ ì‚¬ë„ ë²”ìœ„: {similarities.min():.4f} ~ {similarities.max():.4f}")
            print(f"[DEBUG] í‰ê·  ìœ ì‚¬ë„: {similarities.mean():.4f}")

            # ìƒìœ„ Kê°œ ê²°ê³¼
            if len(similarities) > 0:
                top_indices = np.argsort(similarities)[-top_k:][::-1]
                print(f"[DEBUG] ìƒìœ„ {top_k}ê°œ ìœ ì‚¬ë„:")
                for i, idx in enumerate(top_indices, 1):
                    print(f"  {i}. ì¸ë±ìŠ¤ {idx}: {similarities[idx]:.4f}")

        except Exception as e:
            print(f"[!] ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    def encode_text(self, text):
        """get_text_embeddingì˜ ë³„ì¹­"""
        return self.get_text_embedding(text)

    def get_image_embedding(self, image_path):
        """encode_imageì˜ ë³„ì¹­"""
        return self.encode_image(image_path)

    def encode_all_images(self, metadata, image_root):
        embeddings = []
        filenames = []
        for item in metadata:
            image_path = os.path.join(image_root, item['filepath'])
            try:
                emb = self.encode_image(image_path)
                embeddings.append(emb)
                filenames.append(item['filename'])
            except Exception as e:
                print(f"[ERROR] {image_path}: {e}")
        return embeddings, filenames

    def preprocess_korean_dialogue(self, text):
        """í•œêµ­ì–´ ëŒ€í™” ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£.,!?:-]', '', text)

        # ê¸¸ì´ ì œí•œ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        if len(text) > 150:
            sentences = text.split('.')
            if len(sentences) > 2:
                text = '.'.join(sentences[:2]) + '.'
            else:
                text = text[:150] + '...'

        return text.strip()

    def test_tokenization(self, text):
        """í† í°í™” í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ (ë””ë²„ê¹…ìš©)"""
        try:
            processed_text = self.truncate_korean_text(text)
            tokens = clip.tokenize([processed_text], truncate=True)
            print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text}")
            print(f"ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {processed_text}")
            print(f"í† í° ìˆ˜: {tokens.shape[1]}")
            print(f"í† í°í™” ì„±ê³µ: True")
            return True
        except Exception as e:
            print(f"í† í°í™” ì‹¤íŒ¨: {e}")
            return False

    def validate_embeddings(self, text_emb, image_embeddings):
        """ì„ë² ë”© ìœ íš¨ì„± ê²€ì¦"""
        issues = []

        # í…ìŠ¤íŠ¸ ì„ë² ë”© ê²€ì¦
        if text_emb is None:
            issues.append("í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ Noneì…ë‹ˆë‹¤")
        elif len(text_emb) == 0:
            issues.append("í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        elif np.any(np.isnan(text_emb)):
            issues.append("í…ìŠ¤íŠ¸ ì„ë² ë”©ì— NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤")
        elif np.any(np.isinf(text_emb)):
            issues.append("í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ë¬´í•œëŒ€ ê°’ì´ ìˆìŠµë‹ˆë‹¤")

        # ì´ë¯¸ì§€ ì„ë² ë”© ê²€ì¦
        if len(image_embeddings) == 0:
            issues.append("ì´ë¯¸ì§€ ì„ë² ë”©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        elif np.any(np.isnan(image_embeddings)):
            issues.append("ì´ë¯¸ì§€ ì„ë² ë”©ì— NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤")
        elif np.any(np.isinf(image_embeddings)):
            issues.append("ì´ë¯¸ì§€ ì„ë² ë”©ì— ë¬´í•œëŒ€ ê°’ì´ ìˆìŠµë‹ˆë‹¤")

        if issues:
            print(f"[!] ì„ë² ë”© ê²€ì¦ ì‹¤íŒ¨:")
            for issue in issues:
                print(f"  - {issue}")
            return False

        return True

    def get_embedding_stats(self, text_emb, image_embeddings):
        """ì„ë² ë”© í†µê³„ ì •ë³´ ì¶œë ¥"""
        try:
            print(f"[STATS] í…ìŠ¤íŠ¸ ì„ë² ë”©:")
            print(f"  - ì°¨ì›: {text_emb.shape}")
            print(f"  - í‰ê· : {np.mean(text_emb):.4f}")
            print(f"  - í‘œì¤€í¸ì°¨: {np.std(text_emb):.4f}")
            print(f"  - ë²”ìœ„: {np.min(text_emb):.4f} ~ {np.max(text_emb):.4f}")

            print(f"[STATS] ì´ë¯¸ì§€ ì„ë² ë”©:")
            print(f"  - ì°¨ì›: {image_embeddings.shape}")
            print(f"  - í‰ê· : {np.mean(image_embeddings):.4f}")
            print(f"  - í‘œì¤€í¸ì°¨: {np.std(image_embeddings):.4f}")
            print(f"  - ë²”ìœ„: {np.min(image_embeddings):.4f} ~ {np.max(image_embeddings):.4f}")

        except Exception as e:
            print(f"[!] í†µê³„ ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨: {e}")

    def similarity_analysis(self, text, emotions, situations, image_embeddings, image_files, top_k=10):
        """
        ì¢…í•©ì ì¸ ìœ ì‚¬ë„ ë¶„ì„ ë° ê²°ê³¼ ì¶œë ¥

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            emotions: ê°ì§€ëœ ê°ì • ë¦¬ìŠ¤íŠ¸
            situations: ê°ì§€ëœ ìƒí™© ë¦¬ìŠ¤íŠ¸
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´
            image_files: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼ ë¶„ì„

        Returns:
            analysis_results: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            text_emb = self.get_text_embedding(text)

            # ë“€ì–¼ ìœ ì‚¬ë„ ê³„ì‚°
            combined_similarities, text_similarities, weighted_similarities = self.get_dual_similarities(
                text_emb, image_embeddings, emotions, situations
            )

            # Top-K ë¶„ì„
            top_indices = np.argsort(combined_similarities)[-top_k:][::-1]

            analysis_results = {
                'input_text': text,
                'emotions': emotions,
                'situations': situations,
                'top_matches': [],
                'similarity_stats': {
                    'combined_avg': float(np.mean(combined_similarities)),
                    'combined_max': float(np.max(combined_similarities)),
                    'combined_min': float(np.min(combined_similarities)),
                    'text_avg': float(np.mean(text_similarities)),
                    'weighted_avg': float(np.mean(weighted_similarities))
                }
            }

            # Top-K ê²°ê³¼ ì €ì¥
            for i, idx in enumerate(top_indices, 1):
                if idx < len(image_files):
                    analysis_results['top_matches'].append({
                        'rank': i,
                        'image_file': os.path.basename(image_files[idx]),
                        'combined_score': float(combined_similarities[idx]),
                        'text_score': float(text_similarities[idx]),
                        'weighted_score': float(weighted_similarities[idx])
                    })

            return analysis_results

        except Exception as e:
            print(f"[!] ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'input_text': text,
                'emotions': emotions or [],
                'situations': situations or [],
                'top_matches': [],
                'similarity_stats': {},
                'error': str(e)
            }

    def optimize_similarity_computation(self, text_emb, image_embeddings):
        """
        ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™” (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)

        Args:
            text_emb: í…ìŠ¤íŠ¸ ì„ë² ë”©
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´

        Returns:
            optimized_similarities: ìµœì í™”ëœ ìœ ì‚¬ë„ ì ìˆ˜
        """
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸° ì¡°ì •
            n_images = len(image_embeddings)

            if n_images < 1000:
                # ì‘ì€ ë°ì´í„°ì…‹: í•œ ë²ˆì— ê³„ì‚°
                return self._calculate_text_similarity(text_emb, image_embeddings)
            elif n_images < 10000:
                # ì¤‘ê°„ ë°ì´í„°ì…‹: ë°°ì¹˜ ì²˜ë¦¬
                return self.batch_text_similarity(text_emb, image_embeddings, batch_size=500)
            else:
                # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹: ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
                return self.batch_text_similarity(text_emb, image_embeddings, batch_size=100)

        except Exception as e:
            print(f"[!] ìµœì í™”ëœ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros(len(image_embeddings))

    def cache_embeddings(self, texts, cache_size=1000):
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìºì‹± ì‹œìŠ¤í…œ

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            cache_size: ìºì‹œ ìµœëŒ€ í¬ê¸°

        Returns:
            embedding_cache: ìºì‹œëœ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        """
        try:
            import hashlib
            from collections import OrderedDict

            embedding_cache = OrderedDict()

            for text in texts:
                # í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

                if text_hash not in embedding_cache:
                    # ìºì‹œ í¬ê¸° ì œí•œ
                    if len(embedding_cache) >= cache_size:
                        embedding_cache.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°

                    # ì„ë² ë”© ìƒì„± ë° ìºì‹œ ì €ì¥
                    embedding = self.get_text_embedding(text)
                    embedding_cache[text_hash] = {
                        'text': text,
                        'embedding': embedding
                    }

            return embedding_cache

        except Exception as e:
            print(f"[!] ì„ë² ë”© ìºì‹± ì‹¤íŒ¨: {e}")
            return {}

    def compare_similarity_methods(self, text, image_embeddings, emotions=None, situations=None):
        """
        ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ê³„ì‚° ë°©ë²• ë¹„êµ

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            image_embeddings: ì´ë¯¸ì§€ ì„ë² ë”© ë°°ì—´
            emotions: ê°ì • ë¦¬ìŠ¤íŠ¸
            situations: ìƒí™© ë¦¬ìŠ¤íŠ¸

        Returns:
            comparison_results: ë¹„êµ ê²°ê³¼
        """
        try:
            text_emb = self.get_text_embedding(text)

            # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„
            text_similarities = self._calculate_text_similarity(text_emb, image_embeddings)

            # 2. ê°•í™”ëœ ì„ë² ë”© ìœ ì‚¬ë„
            enhanced_emb = self.get_enhanced_text_embedding(text, emotions, situations)
            enhanced_similarities = self._calculate_text_similarity(enhanced_emb, image_embeddings)

            # 3. ë“€ì–¼ ìœ ì‚¬ë„
            dual_similarities, _, _ = self.get_dual_similarities(text_emb, image_embeddings, emotions, situations)

            # 4. ë°°ì¹˜ ìœ ì‚¬ë„ (ì„±ëŠ¥ ë¹„êµìš©)
            batch_similarities = self.batch_text_similarity(text_emb, image_embeddings)

            # ê²°ê³¼ ë¹„êµ
            comparison_results = {
                'text_method': {
                    'max_score': float(np.max(text_similarities)),
                    'avg_score': float(np.mean(text_similarities)),
                    'top_index': int(np.argmax(text_similarities))
                },
                'enhanced_method': {
                    'max_score': float(np.max(enhanced_similarities)),
                    'avg_score': float(np.mean(enhanced_similarities)),
                    'top_index': int(np.argmax(enhanced_similarities))
                },
                'dual_method': {
                    'max_score': float(np.max(dual_similarities)),
                    'avg_score': float(np.mean(dual_similarities)),
                    'top_index': int(np.argmax(dual_similarities))
                },
                'batch_method': {
                    'max_score': float(np.max(batch_similarities)),
                    'avg_score': float(np.mean(batch_similarities)),
                    'top_index': int(np.argmax(batch_similarities))
                }
            }

            return comparison_results

        except Exception as e:
            print(f"[!] ìœ ì‚¬ë„ ë°©ë²• ë¹„êµ ì‹¤íŒ¨: {e}")
            return {}

    def health_check(self):
        """
        CLIPEncoder ìƒíƒœ ì ê²€

        Returns:
            health_status: ìƒíƒœ ì ê²€ ê²°ê³¼
        """
        try:
            health_status = {
                'model_loaded': False,
                'device_available': False,
                'tokenization_working': False,
                'embedding_working': False,
                'similarity_working': False,
                'issues': []
            }

            # 1. ëª¨ë¸ ë¡œë“œ í™•ì¸
            if hasattr(self, 'model') and self.model is not None:
                health_status['model_loaded'] = True
            else:
                health_status['issues'].append("CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")

            # 2. ë””ë°”ì´ìŠ¤ í™•ì¸
            if hasattr(self, 'device'):
                health_status['device_available'] = True
                health_status['device'] = str(self.device)
            else:
                health_status['issues'].append("ë””ë°”ì´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

            # 3. í† í°í™” í…ŒìŠ¤íŠ¸
            try:
                test_result = self.test_tokenization("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
                health_status['tokenization_working'] = test_result
                if not test_result:
                    health_status['issues'].append("í† í°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            except:
                health_status['issues'].append("í† í°í™” í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ")

            # 4. ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
            try:
                test_embedding = self.get_text_embedding("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸")
                if test_embedding is not None and len(test_embedding) > 0:
                    health_status['embedding_working'] = True
                else:
                    health_status['issues'].append("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            except:
                health_status['issues'].append("ì„ë² ë”© ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ")

            # 5. ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
            try:
                test_emb = np.random.rand(512)
                test_img_embs = np.random.rand(5, 512)
                similarities = self._calculate_text_similarity(test_emb, test_img_embs)
                if len(similarities) == 5:
                    health_status['similarity_working'] = True
                else:
                    health_status['issues'].append("ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ ì°¨ì› ì˜¤ë¥˜")
            except:
                health_status['issues'].append("ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜ˆì™¸ ë°œìƒ")

            # ì „ì²´ ìƒíƒœ í‰ê°€
            working_components = sum([
                health_status['model_loaded'],
                health_status['device_available'],
                health_status['tokenization_working'],
                health_status['embedding_working'],
                health_status['similarity_working']
            ])

            health_status['overall_health'] = working_components / 5.0
            health_status['status'] = 'HEALTHY' if working_components >= 4 else 'ISSUES_DETECTED'

            return health_status

        except Exception as e:
            return {
                'status': 'CRITICAL_ERROR',
                'error': str(e),
                'issues': [f"ìƒíƒœ ì ê²€ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}"]
            }

    def print_health_report(self):
        """ìƒíƒœ ì ê²€ ê²°ê³¼ ì¶œë ¥"""
        health = self.health_check()

        print("\n" + "=" * 50)
        print("ğŸ” CLIPEncoder ìƒíƒœ ì ê²€ ê²°ê³¼")
        print("=" * 50)

        print(f"ì „ì²´ ìƒíƒœ: {health['status']}")
        print(f"ê±´ê°•ë„: {health.get('overall_health', 0):.1%}")

        print(f"\nêµ¬ì„± ìš”ì†Œ ìƒíƒœ:")
        print(f"  âœ… ëª¨ë¸ ë¡œë“œ: {'OK' if health['model_loaded'] else 'âŒ FAIL'}")
        print(f"  âœ… ë””ë°”ì´ìŠ¤: {'OK' if health['device_available'] else 'âŒ FAIL'}")
        if 'device' in health:
            print(f"     ë””ë°”ì´ìŠ¤: {health['device']}")
        print(f"  âœ… í† í°í™”: {'OK' if health['tokenization_working'] else 'âŒ FAIL'}")
        print(f"  âœ… ì„ë² ë”©: {'OK' if health['embedding_working'] else 'âŒ FAIL'}")
        print(f"  âœ… ìœ ì‚¬ë„: {'OK' if health['similarity_working'] else 'âŒ FAIL'}")

        if health['issues']:
            print(f"\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in health['issues']:
                print(f"  - {issue}")
        else:
            print(f"\nğŸ‰ ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ ì •ìƒ ì‘ë™ ì¤‘!")

        print("=" * 50)