import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import json
import os


class MemeRecommendationEvaluator:
    def __init__(self, recommender, test_data):
        self.recommender = recommender
        self.test_data = test_data
        self.evaluation_results = []

    def evaluate_recommendations(self):
        """ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ - ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”"""
        if not self.test_data:
            print("[!] í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'total_tests': 0,
                'accuracy_top1': 0.0,
                'accuracy_top3': 0.0,
                'accuracy_top5': 0.0,
                'emotion_accuracy': {},
                'situation_accuracy': {},
                'detailed_results': []
            }

        correct_top1 = 0
        correct_top3 = 0
        correct_top5 = 0
        total_tests = len(self.test_data)

        # ZeroDivisionError ë°©ì§€
        if total_tests == 0:
            print("[!] í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return {
                'total_tests': 0,
                'accuracy_top1': 0.0,
                'accuracy_top3': 0.0,
                'accuracy_top5': 0.0,
                'emotion_accuracy': {},
                'situation_accuracy': {},
                'detailed_results': []
            }

        emotion_accuracy = {'ë¶„ë…¸': [], 'ê¸°ì¨': [], 'ìŠ¬í””': [], 'ë¶ˆì•ˆ': [], 'ê°ì‚¬': [], 'ìì‹ ': []}
        situation_accuracy = {'ì§ì¥': [], 'ì§„ë¡œ': [], 'ì¸ê°„ê´€ê³„': [], 'ì—°ì• ': [], 'ëˆ': [], 'ê°œì¸ì ': []}

        print("[*] ì„±ëŠ¥ í‰ê°€ ì‹œì‘...")

        for i, test_case in enumerate(self.test_data):
            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{total_tests}")

            try:
                # ì¶”ì²œ ì‹¤í–‰
                result = self.recommender.recommend(test_case['text'])

                # ì‹¤ì œ ì •ë‹µ ì´ë¯¸ì§€ë“¤ (ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ)
                expected_images = [test_case.get('image_filename', '')]
                if not expected_images[0]:  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°
                    expected_images = []

                # Top-K ì •í™•ë„ ê³„ì‚°
                if result and 'top_5' in result and result['top_5']:
                    top_5_images = [item.get('filename', '') for item in result['top_5']]
                else:
                    top_5_images = [result.get('best_image', '')] if result else []

                # ì˜ˆìƒ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ ì •í™•ë„ ê³„ì‚°
                if expected_images and expected_images[0]:
                    is_top1_correct = result.get('best_image', '') in expected_images
                    is_top3_correct = any(img in expected_images for img in top_5_images[:3])
                    is_top5_correct = any(img in expected_images for img in top_5_images)

                    if is_top1_correct:
                        correct_top1 += 1
                    if is_top3_correct:
                        correct_top3 += 1
                    if is_top5_correct:
                        correct_top5 += 1
                else:
                    # ì˜ˆìƒ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°, ì¶”ì²œì´ ì„±ê³µí–ˆë‹¤ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                    is_top1_correct = result is not None
                    is_top3_correct = result is not None
                    is_top5_correct = result is not None

                    if is_top1_correct:
                        correct_top1 += 1
                        correct_top3 += 1
                        correct_top5 += 1

                # ê°ì •ë³„ ì •í™•ë„
                test_emotion = test_case.get('emotion', '')
                if test_emotion and test_emotion in emotion_accuracy:
                    emotion_accuracy[test_emotion].append(is_top1_correct)

                # ìƒí™©ë³„ ì •í™•ë„
                test_situations = test_case.get('situation', [])
                if isinstance(test_situations, str):
                    test_situations = [test_situations]

                for situation in test_situations:
                    if situation in situation_accuracy:
                        situation_accuracy[situation].append(is_top1_correct)

                # ê°œë³„ ê²°ê³¼ ì €ì¥
                self.evaluation_results.append({
                    'dialogue_id': test_case.get('dialogue_id', f'test_{i}'),
                    'text': test_case['text'][:100] + "..." if len(test_case['text']) > 100 else test_case['text'],
                    'expected_image': expected_images[0] if expected_images else 'N/A',
                    'predicted_image': result.get('best_image', 'N/A') if result else 'N/A',
                    'score': result.get('score', 0.0) if result else 0.0,
                    'emotion': test_emotion,
                    'top1_correct': is_top1_correct,
                    'top3_correct': is_top3_correct,
                    'top5_correct': is_top5_correct
                })

            except Exception as e:
                print(f"[!] í‰ê°€ ì¤‘ ì˜¤ë¥˜ (í…ŒìŠ¤íŠ¸ {i}): {e}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
                self.evaluation_results.append({
                    'dialogue_id': test_case.get('dialogue_id', f'test_{i}'),
                    'text': test_case.get('text', 'N/A')[:100] + "...",
                    'expected_image': 'N/A',
                    'predicted_image': 'ERROR',
                    'score': 0.0,
                    'emotion': test_case.get('emotion', ''),
                    'top1_correct': False,
                    'top3_correct': False,
                    'top5_correct': False,
                    'error': str(e)
                })

        # ì „ì²´ ì •í™•ë„ ê³„ì‚° (ZeroDivisionError ë°©ì§€)
        accuracy_top1 = correct_top1 / total_tests if total_tests > 0 else 0.0
        accuracy_top3 = correct_top3 / total_tests if total_tests > 0 else 0.0
        accuracy_top5 = correct_top5 / total_tests if total_tests > 0 else 0.0

        # ê°ì •ë³„ ì •í™•ë„ ê³„ì‚°
        emotion_results = {}
        for emotion, results in emotion_accuracy.items():
            if results:
                emotion_results[emotion] = sum(results) / len(results)
            else:
                emotion_results[emotion] = 0.0

        # ìƒí™©ë³„ ì •í™•ë„ ê³„ì‚°
        situation_results = {}
        for situation, results in situation_accuracy.items():
            if results:
                situation_results[situation] = sum(results) / len(results)
            else:
                situation_results[situation] = 0.0

        return {
            'total_tests': total_tests,
            'accuracy_top1': accuracy_top1,
            'accuracy_top3': accuracy_top3,
            'accuracy_top5': accuracy_top5,
            'emotion_accuracy': emotion_results,
            'situation_accuracy': situation_results,
            'detailed_results': self.evaluation_results
        }

    def print_evaluation_report(self, results):
        """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("=" * 60)

        print(f"ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {results['total_tests']}ê°œ")

        if results['total_tests'] > 0:
            print(f"Top-1 ì •í™•ë„: {results['accuracy_top1']:.3f} ({results['accuracy_top1'] * 100:.1f}%)")
            print(f"Top-3 ì •í™•ë„: {results['accuracy_top3']:.3f} ({results['accuracy_top3'] * 100:.1f}%)")
            print(f"Top-5 ì •í™•ë„: {results['accuracy_top5']:.3f} ({results['accuracy_top5'] * 100:.1f}%)")

            print("\nğŸ“ˆ ê°ì •ë³„ ì •í™•ë„:")
            emotion_items = [(k, v) for k, v in results['emotion_accuracy'].items() if v > 0]
            if emotion_items:
                for emotion, accuracy in emotion_items:
                    print(f"  {emotion}: {accuracy:.3f} ({accuracy * 100:.1f}%)")
            else:
                print("  ê°ì •ë³„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            print("\nğŸ¢ ìƒí™©ë³„ ì •í™•ë„:")
            situation_items = [(k, v) for k, v in results['situation_accuracy'].items() if v > 0]
            if situation_items:
                for situation, accuracy in situation_items:
                    print(f"  {situation}: {accuracy:.3f} ({accuracy * 100:.1f}%)")
            else:
                print("  ìƒí™©ë³„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
            failed_cases = [r for r in results['detailed_results'] if not r.get('top1_correct', False)]
            if failed_cases and len(failed_cases) <= 10:  # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ê°€ 10ê°œ ì´í•˜ì¼ ë•Œë§Œ í‘œì‹œ
                print(f"\nâŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ (ì´ {len(failed_cases)}ê°œ):")
                for i, case in enumerate(failed_cases[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    print(f"  {i}. í…ìŠ¤íŠ¸: {case['text']}")
                    if case.get('error'):
                        print(f"     ì˜¤ë¥˜: {case['error']}")
                    else:
                        print(f"     ì˜ˆìƒ: {case['expected_image']}, ì˜ˆì¸¡: {case['predicted_image']}")
                        print(f"     ì ìˆ˜: {case['score']:.3f}, ê°ì •: {case['emotion']}")
                    print()
            elif len(failed_cases) > 10:
                print(f"\nâŒ ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {len(failed_cases)}ê°œ (ë„ˆë¬´ ë§ì•„ ìƒì„¸ í‘œì‹œ ìƒëµ)")
        else:
            print("âš ï¸ í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def save_evaluation_results(self, results, filename="evaluation_results.json"):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # results ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            results_dir = "results"
            if not os.path.exists(results_dir):
                os.makedirs(results_dir)
                print(f"[*] {results_dir} ë””ë ‰í† ë¦¬ ìƒì„±")

            # ì „ì²´ ê²½ë¡œ êµ¬ì„±
            full_path = os.path.join(results_dir, filename)

            # numpy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj

            # ê²°ê³¼ ë³€í™˜ ë° ì €ì¥
            converted_results = convert_numpy_types(results)

            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(converted_results, f, ensure_ascii=False, indent=2)
            print(f"[*] í‰ê°€ ê²°ê³¼ ì €ì¥: {full_path}")

        except Exception as e:
            print(f"[!] í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def calculate_confusion_matrix(self, results):
        """í˜¼ë™ í–‰ë ¬ ê³„ì‚°"""
        if not results['detailed_results']:
            return None

        try:
            # ê°ì •ë³„ í˜¼ë™ í–‰ë ¬ ë°ì´í„° ìˆ˜ì§‘
            emotion_predictions = []
            emotion_actuals = []

            for result in results['detailed_results']:
                actual_emotion = result.get('emotion', '')
                # ì˜ˆì¸¡ëœ ê°ì •ì€ ê°„ë‹¨íˆ ì„±ê³µ/ì‹¤íŒ¨ë¡œ êµ¬ë¶„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                predicted_emotion = actual_emotion if result.get('top1_correct', False) else 'other'

                if actual_emotion:  # ì‹¤ì œ ê°ì •ì´ ìˆëŠ” ê²½ìš°ë§Œ
                    emotion_actuals.append(actual_emotion)
                    emotion_predictions.append(predicted_emotion)

            return {
                'emotion_actuals': emotion_actuals,
                'emotion_predictions': emotion_predictions
            }

        except Exception as e:
            print(f"[!] í˜¼ë™ í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None

    def get_summary_statistics(self, results):
        """ìš”ì•½ í†µê³„ ê³„ì‚°"""
        if not results['detailed_results']:
            return {}

        try:
            scores = [r.get('score', 0.0) for r in results['detailed_results']]

            return {
                'mean_score': np.mean(scores) if scores else 0.0,
                'median_score': np.median(scores) if scores else 0.0,
                'std_score': np.std(scores) if scores else 0.0,
                'min_score': np.min(scores) if scores else 0.0,
                'max_score': np.max(scores) if scores else 0.0,
                'total_errors': len([r for r in results['detailed_results'] if 'error' in r]),
                'success_rate': results['accuracy_top1']
            }

        except Exception as e:
            print(f"[!] í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Evaluator í…ŒìŠ¤íŠ¸")


    # ê°€ì§œ ì¶”ì²œ ì‹œìŠ¤í…œ
    class MockRecommender:
        def recommend(self, text):
            return {
                'best_image': 'test_image.jpg',
                'score': 0.5,
                'emotions': ['ê¸°ì¨'],
                'situations': ['ì¼ìƒ'],
                'top_5': [
                    {'filename': 'test_image.jpg', 'score': 0.5},
                    {'filename': 'test_image2.jpg', 'score': 0.4},
                    {'filename': 'test_image3.jpg', 'score': 0.3},
                    {'filename': 'test_image4.jpg', 'score': 0.2},
                    {'filename': 'test_image5.jpg', 'score': 0.1}
                ]
            }


    # ê°€ì§œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    mock_test_data = [
        {
            'dialogue_id': 'test_1',
            'text': 'ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì•„!',
            'emotion': 'ê¸°ì¨',
            'situation': ['ì¼ìƒ'],
            'image_filename': 'test_image.jpg'
        },
        {
            'dialogue_id': 'test_2',
            'text': 'ë„ˆë¬´ ìŠ¬í¼',
            'emotion': 'ìŠ¬í””',
            'situation': ['ê°œì¸ì '],
            'image_filename': 'sad_image.jpg'
        }
    ]

    # í‰ê°€ ì‹¤í–‰
    mock_recommender = MockRecommender()
    evaluator = MemeRecommendationEvaluator(mock_recommender, mock_test_data)
    results = evaluator.evaluate_recommendations()

    # ê²°ê³¼ ì¶œë ¥
    evaluator.print_evaluation_report(results)

    # í†µê³„ ì •ë³´
    stats = evaluator.get_summary_statistics(results)
    print(f"\nğŸ“Š ìš”ì•½ í†µê³„: {stats}")

    print("\nâœ… Evaluator í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")