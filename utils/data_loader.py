import json
import os
import numpy as np
from sklearn.model_selection import train_test_split


class DialogueDataLoader:
    def __init__(self, dialogue_metadata_file="data/converted_dialogues.json",
                 image_metadata_file="data/image_metadata.json"):
        self.dialogue_data = []
        self.image_data = []

        # ëŒ€í™” ë©”íƒ€ë°ì´í„° ë¡œë“œ
        if os.path.exists(dialogue_metadata_file):
            with open(dialogue_metadata_file, 'r', encoding='utf-8') as f:
                self.dialogue_data = json.load(f)
            print(f"[*] ëŒ€í™” ë°ì´í„° ë¡œë“œ: {len(self.dialogue_data)}ê°œ")
        else:
            print(f"[!] ëŒ€í™” ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dialogue_metadata_file}")

        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        if os.path.exists(image_metadata_file):
            with open(image_metadata_file, 'r', encoding='utf-8') as f:
                self.image_data = json.load(f)
            print(f"[*] ì´ë¯¸ì§€ ë°ì´í„° ë¡œë“œ: {len(self.image_data)}ê°œ")
        else:
            print(f"[!] ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_metadata_file}")

    def get_training_pairs(self):
        """í•™ìŠµìš© (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€) ìŒ ìƒì„±"""
        training_pairs = []

        for dialogue in self.dialogue_data:
            # ì „ì²´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            full_context = " ".join(dialogue.get('context', []))
            utterance = dialogue.get('utterance', '')
            full_text = f"{full_context} {utterance}".strip()

            # ëŒ€í™” ì •ë³´
            dialogue_emotion = dialogue.get('emotion', '')
            dialogue_situation = dialogue.get('situation', [])
            dialogue_tags = dialogue.get('image_tags', [])

            # ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ë“¤ ì°¾ê¸°
            matching_images = self.find_matching_images(
                dialogue_emotion, dialogue_situation, dialogue_tags
            )

            for img_info in matching_images:
                training_pairs.append({
                    'dialogue_id': dialogue.get('dialogue_id'),
                    'text': full_text,
                    'emotion': dialogue_emotion,
                    'situation': dialogue_situation,
                    'tags': dialogue_tags,
                    'image_filename': img_info['filename'],
                    'image_path': img_info['path'],
                    'image_category': img_info['category'],
                    'image_subcategory': img_info['subcategory'],
                    'relevance_score': img_info['relevance_score']
                })

        print(f"[*] í•™ìŠµ ìŒ ìƒì„± ì™„ë£Œ: {len(training_pairs)}ê°œ")
        return training_pairs

    def find_matching_images(self, emotion, situations, tags):
        """ê°ì •, ìƒí™©, íƒœê·¸ ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ ì°¾ê¸°"""
        matching_images = []

        # ê°ì • ë§¤í•‘ í…Œì´ë¸”
        emotion_mapping = {
            'ë¶„ë…¸': ['ë¶„ë…¸', 'í™”ë‚¨', 'ì§œì¦', 'ë…¸ì—¬ì›Œ'],
            'ê¸°ì¨': ['ê¸°ì¨', 'í–‰ë³µ', 'ì¦ê±°ì›€', 'ë§Œì¡±', 'ê°ì‚¬'],
            'ìŠ¬í””': ['ìŠ¬í””', 'ìš°ìš¸', 'ìƒì²˜', 'ì„œëŸ¬ì›€'],
            'ë¶ˆì•ˆ': ['ë¶ˆì•ˆ', 'ê±±ì •', 'ë‘ë ¤ì›€', 'ë‹¹í™©'],
            'ë†€ëŒ': ['ë†€ëŒ', 'ë‹¹í™©', 'ì¶©ê²©'],
            'í˜ì˜¤': ['í˜ì˜¤', 'ì‹«ìŒ', 'ì—­ê²¨ì›€']
        }

        # ìƒí™© ë§¤í•‘ í…Œì´ë¸”
        situation_mapping = {
            'ì§ì¥': ['ì—…ë¬´', 'ì§ì¥', 'íšŒì‚¬'],
            'ì§„ë¡œ': ['ì—…ë¬´', 'ì§ì¥', 'ì§„ë¡œ'],
            'ì·¨ì—…': ['ì—…ë¬´', 'ì§ì¥', 'ì§„ë¡œ'],
            'ì—°ì• ': ['ì—°ì• ', 'ì‚¬ë‘', 'ì¹œêµ¬'],
            'ì¸ê°„ê´€ê³„': ['ì¹œêµ¬', 'ê´€ê³„', 'ì†Œí†µ'],
            'ê°€ì¡±': ['ê°€ì¡±', 'ì¹œêµ¬'],
            'ëˆ': ['ê²½ì œ', 'ëˆ', 'ìƒí™œ']
        }

        for img in self.image_data:
            if img.get('processing_status') != 'success':
                continue

            relevance_score = 0.0

            # 1. ê°ì • ë§¤ì¹­ (ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ)
            img_category = img.get('category', '').lower()
            img_subcategory = img.get('subcategory', '').lower()
            img_tags = [tag.lower() for tag in img.get('tags', [])]

            # ì§ì ‘ ê°ì • ë§¤ì¹­
            if emotion.lower() in img_category or emotion.lower() in img_subcategory:
                relevance_score += 2.0

            # ê°ì • ë§¤í•‘ í…Œì´ë¸”ì„ í†µí•œ ë§¤ì¹­
            emotion_keywords = emotion_mapping.get(emotion, [])
            for keyword in emotion_keywords:
                if keyword in img_category or keyword in img_subcategory:
                    relevance_score += 1.5
                    break

            # 2. íƒœê·¸ ë§¤ì¹­
            for tag in tags:
                tag_lower = tag.lower()
                if tag_lower in img_tags:
                    relevance_score += 1.0
                elif any(tag_lower in img_tag for img_tag in img_tags):
                    relevance_score += 0.8

            # 3. ì„œë¸Œì¹´í…Œê³ ë¦¬ì™€ íƒœê·¸ ë§¤ì¹­
            for tag in tags:
                if tag.lower() in img_subcategory:
                    relevance_score += 0.8

            # 4. ìƒí™© ë§¤ì¹­
            img_context = [ctx.lower() for ctx in img.get('usage_context', [])]
            for situation in situations:
                situation_keywords = situation_mapping.get(situation, [situation])
                for keyword in situation_keywords:
                    if keyword.lower() in img_context:
                        relevance_score += 0.6
                        break

            # 5. í…ìŠ¤íŠ¸ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
            description = img.get('text_description', '').lower()

            # ê°ì • ê´€ë ¨ í‚¤ì›Œë“œê°€ ì„¤ëª…ì— ìˆëŠ”ì§€ í™•ì¸
            for keyword in emotion_keywords:
                if keyword in description:
                    relevance_score += 0.3
                    break

            # íƒœê·¸ê°€ ì„¤ëª…ì— ìˆëŠ”ì§€ í™•ì¸
            for tag in tags:
                if tag.lower() in description:
                    relevance_score += 0.2

            # ê´€ë ¨ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€ë§Œ í¬í•¨ (ìµœì†Œ ì„ê³„ê°’)
            if relevance_score >= 0.5:
                img_path = img.get('processed_path') or img.get('filepath', '')

                # ê²½ë¡œ ì •ê·œí™”
                if img_path and not os.path.exists(img_path):
                    # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
                    relative_path = f"data/images/{img.get('filepath', '')}"
                    if os.path.exists(relative_path):
                        img_path = relative_path

                if img_path and os.path.exists(img_path):
                    matching_images.append({
                        'filename': img['filename'],
                        'path': img_path,
                        'category': img.get('category', ''),
                        'subcategory': img.get('subcategory', ''),
                        'tags': img.get('tags', []),
                        'relevance_score': relevance_score
                    })

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        matching_images.sort(key=lambda x: x['relevance_score'], reverse=True)

        # ìƒìœ„ ì´ë¯¸ì§€ë“¤ë§Œ ë°˜í™˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ìƒìœ„ 10ê°œ)
        max_images = min(10, len(matching_images))
        return matching_images[:max_images]

    def split_data(self, test_size=0.2, random_state=42):
        """ë°ì´í„°ë¥¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• """
        training_pairs = self.get_training_pairs()

        if len(training_pairs) == 0:
            print("[!] í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return [], []

        # ëŒ€í™” IDë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¶„í•  (ê°™ì€ ëŒ€í™”ê°€ train/testì— ë™ì‹œì— ë“¤ì–´ê°€ì§€ ì•Šë„ë¡)
        dialogue_groups = {}
        for pair in training_pairs:
            dialogue_id = pair['dialogue_id']
            if dialogue_id not in dialogue_groups:
                dialogue_groups[dialogue_id] = []
            dialogue_groups[dialogue_id].append(pair)

        # ëŒ€í™” ID ë¦¬ìŠ¤íŠ¸
        dialogue_ids = list(dialogue_groups.keys())

        # ëŒ€í™” ID ê¸°ì¤€ìœ¼ë¡œ train/test ë¶„í• 
        train_ids, test_ids = train_test_split(
            dialogue_ids, test_size=test_size, random_state=random_state
        )

        # ë¶„í• ëœ IDì— ë”°ë¼ ì‹¤ì œ ë°ì´í„° ë¶„í• 
        train_data = []
        test_data = []

        for dialogue_id in train_ids:
            train_data.extend(dialogue_groups[dialogue_id])

        for dialogue_id in test_ids:
            test_data.extend(dialogue_groups[dialogue_id])

        print(f"[*] ë°ì´í„° ë¶„í•  ì™„ë£Œ")
        print(f"    - í•™ìŠµ ëŒ€í™”: {len(train_ids)}ê°œ, í•™ìŠµ ìŒ: {len(train_data)}ê°œ")
        print(f"    - í…ŒìŠ¤íŠ¸ ëŒ€í™”: {len(test_ids)}ê°œ, í…ŒìŠ¤íŠ¸ ìŒ: {len(test_data)}ê°œ")

        return train_data, test_data

    def get_statistics(self):
        """ë°ì´í„° í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.dialogue_data:
            return {}

        # ê°ì • ë¶„í¬
        emotion_counts = {}
        situation_counts = {}

        for dialogue in self.dialogue_data:
            emotion = dialogue.get('emotion', '')
            if emotion:
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

            situations = dialogue.get('situation', [])
            for situation in situations:
                situation_counts[situation] = situation_counts.get(situation, 0) + 1

        # ì´ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ ë¶„í¬
        category_counts = {}
        for img in self.image_data:
            category = img.get('category', '')
            if category:
                category_counts[category] = category_counts.get(category, 0) + 1

        return {
            'total_dialogues': len(self.dialogue_data),
            'total_images': len(self.image_data),
            'emotion_distribution': emotion_counts,
            'situation_distribution': situation_counts,
            'image_category_distribution': category_counts
        }

    def print_statistics(self):
        """ë°ì´í„° í†µê³„ ì •ë³´ ì¶œë ¥"""
        stats = self.get_statistics()

        print("\n" + "=" * 50)
        print("ğŸ“Š ë°ì´í„° í†µê³„ ì •ë³´")
        print("=" * 50)

        print(f"ì´ ëŒ€í™” ìˆ˜: {stats.get('total_dialogues', 0)}ê°œ")
        print(f"ì´ ì´ë¯¸ì§€ ìˆ˜: {stats.get('total_images', 0)}ê°œ")

        print("\nğŸ˜Š ê°ì • ë¶„í¬:")
        for emotion, count in stats.get('emotion_distribution', {}).items():
            print(f"  {emotion}: {count}ê°œ")

        print("\nğŸ¢ ìƒí™© ë¶„í¬:")
        for situation, count in stats.get('situation_distribution', {}).items():
            print(f"  {situation}: {count}ê°œ")

        print("\nğŸ–¼ï¸ ì´ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for category, count in stats.get('image_category_distribution', {}).items():
            print(f"  {category}: {count}ê°œ")


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸
    loader = DialogueDataLoader()

    # í†µê³„ ì •ë³´ ì¶œë ¥
    loader.print_statistics()

    # í•™ìŠµ ìŒ ìƒì„± í…ŒìŠ¤íŠ¸
    training_pairs = loader.get_training_pairs()

    if training_pairs:
        print(f"\n[í…ŒìŠ¤íŠ¸] ì²« ë²ˆì§¸ í•™ìŠµ ìŒ:")
        first_pair = training_pairs[0]
        for key, value in first_pair.items():
            print(f"  {key}: {value}")

    # ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸
    train_data, test_data = loader.split_data(test_size=0.2)