import os
from models.kobert_encoder import KoBERTEncoder
from models.clip_encoder import CLIPEncoder
from models.matching import find_best_match
from utils.metadata_utils import load_json

import numpy as np

def join_text(context, utterance):
    return " ".join(context + [utterance])

if __name__ == "__main__":
    text_data = load_json("data/converted_dialogues.json")
    image_meta = load_json("data/image_metadata.json")

    kobert = KoBERTEncoder()
    clip_encoder = CLIPEncoder()

    print("[*] ì´ë¯¸ì§€ ì„ë² ë”© ì¤‘...")
    image_embs, image_names = clip_encoder.encode_all_images(image_meta, "data/images")

    for sample in text_data[:5]:  # ì˜ˆì‹œ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
        input_text = join_text(sample["context"], sample["utterance"])
        text_emb = kobert.encode_text(input_text)
        idx, score = find_best_match(text_emb, image_embs)
        print(f"\nğŸ§  ì…ë ¥: {input_text}\nğŸ¯ ì¶”ì²œ: {image_names[idx]} (ìœ ì‚¬ë„: {score:.2f})")
